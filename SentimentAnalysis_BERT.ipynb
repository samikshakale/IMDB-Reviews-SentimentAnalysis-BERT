{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Sentiment Analysis of Movie Reviews using BERT\n",
    "\n",
    "This project aims to train a set of deep learning models on movie review data in order to predict overall sentiment - positive or negative - using BERT, a pretrained NLP model developed by Google.\n",
    "\n",
    "## Overview\n",
    "\n",
    "Initially, I loaded the imdb movie review data using pandas dataframe. Then, I tokenized the reviews and used distilBERT to model the reviews as vectors of length 768, representing the hidden units of the classification token. Finally, I trained the logistic regression, SVM, and Naive Bayes models on this vector data and reported the accuracies of each model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing/Importing Libraries\n",
    "First, I imported all the neccessary modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import transformers as ppb # pytorch transformers\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.dummy import DummyClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Initialization\n",
    "Then, I set a few parameters for training and test data size and for the classification models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 250\n",
    "MAX_ITER = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Dataset\n",
    "Next, I read in the csv file using pandas and stored it into a dataframe object. The dataset should have no header (no column labels), since those can be inferred. I also sliced the dataset to 2*sample_size in order to work with a smaller dataset resulting in shorter program run times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                     0  1\n",
      "0    Match 1: Tag Team Table Match Bubba Ray and Sp...  1\n",
      "1    There's a sign on The Lost Highway that says:<...  1\n",
      "2    (Some spoilers included:)<br /><br />Although,...  1\n",
      "3    Back in the mid/late 80s, an OAV anime by titl...  1\n",
      "4    **Attention Spoilers**<br /><br />First of all...  1\n",
      "..                                                 ... ..\n",
      "564  Title: Zombie 3 (1988) <br /><br />Directors: ...  0\n",
      "565  I have read several reviews that ask the quest...  0\n",
      "566  While filming an 80's horror movie called 'Hot...  0\n",
      "567  I am not surprised to find user comments for t...  0\n",
      "568  The Howling II starts as it means to go on wit...  0\n",
      "\n",
      "[500 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "df_large = pd.read_csv('imdb_original_cleaned.csv', delimiter=',', header=None, encoding='utf-8')\n",
    "df_pos = df_large[df_large[1] == 1]\n",
    "df_pos_small = df_pos[:sample_size]\n",
    "df_neg = df_large[(df_large[1] == 0)]\n",
    "df_neg_small = df_neg[:sample_size]\n",
    "df = pd.concat([df_pos_small,df_neg_small])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "## Preprocessing and Cleaning Dataset\n",
    "Since BERT can only model up to 512 tokens, the reviews in the dataset needed to be shortened. So, the data was cleaned and processed to only include adjectives and adverbs instrumental in determining whether the review is positive or negative. \n",
    "\n",
    "To do this, I first created a function to read files containing lists of common adjectives and adverbs and storing them in a set. Then, I wrote a function to keep only those adjectives and adverbs from each review and applied the function to all the rows of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reads input from a file line by line and returns a set of all the information\n",
    "def SetFromFile(filename):\n",
    "    inp = open(filename, 'r')\n",
    "    adj_set = set()\n",
    "    line = inp.readline().strip('\\n')\n",
    "    while line:\n",
    "        if line != \"review\":\n",
    "            adj_set.add(line)\n",
    "        line = inp.readline().strip('\\n')\n",
    "    return adj_set\n",
    "\n",
    "# Creating the adjective and adverb sets\n",
    "ADJ_SET = SetFromFile(\"large_adjectives.txt\")\n",
    "ADV_SET = SetFromFile(\"large_adverbs.txt\")\n",
    "\n",
    "# Keeps only the words in a given string that are present in the adjective or adverb sets\n",
    "def keep_adj_and_adv(strg, adjSet, advSet):\n",
    "    strg = strg.replace(\"[^a-zA-Z0-9]\", \" \")\n",
    "    str_list = strg.split()\n",
    "    str_set = set(str_list)\n",
    "    setA = str_set.intersection(adjSet)\n",
    "    setB = str_set.intersection(advSet)\n",
    "    final_set = setA.union(setB)\n",
    "    final_list = list(final_set)\n",
    "    final_str = \",\".join(final_list)\n",
    "    return final_str\n",
    "\n",
    "only_adjectives_df = df[0].apply(lambda x: keep_adj_and_adv(x, ADJ_SET, ADV_SET))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Pre-trained BERT Model\n",
    "The next step is to load in the BERT model and tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_class, tokenizer_class, pretrained_weights = (ppb.DistilBertModel, ppb.DistilBertTokenizer, 'distilbert-base-uncased')\n",
    "\n",
    "\n",
    "# Load pretrained model/tokenizer\n",
    "tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
    "model = model_class.from_pretrained(pretrained_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "Then, I tokenized the reviews which consists of breaking up the reviews (containing only adjectives and adverbs) into words and subwords and mapping them to their respective IDs, only keeping the first 512 tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (526 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (645 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (560 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (583 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "# tokenizes the data\n",
    "tokenized = only_adjectives_df.apply(lambda x: tokenizer.encode(x, add_special_tokens=True)[:512])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Padding\n",
    "Right now, tokenized is a list of sentences where each sentence is a list of tokens - thus, a list of lists. However, in order for BERT to process all of the data at once, it needs to be uniform. This can be accomplished by padding the internal lists to the same size. Then, it can be represented as a 2D array instead of a list of lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pads the vectors with 0's\n",
    "max_len = 0\n",
    "for i in tokenized.values:\n",
    "    if len(i) > max_len:\n",
    "        max_len = len(i)\n",
    "        \n",
    "padded = np.array([j + [0]*(max_len-len(j)) for j in tokenized.values])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can view the dimensions of the padded data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 512)\n"
     ]
    }
   ],
   "source": [
    "padded_shape = np.array(padded).shape\n",
    "print(padded_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Masking\n",
    "The final step is to create a mask to ignore the padding put in earlier which will be used in the BERT model when processing the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_mask = np.where(padded != 0, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT Processing\n",
    "First, the padded and attention_mask arrays are turned into tensors and then are inputted into the model() function which creates sentence embeddings. This function will return a tuple where the first value is a 3D array consisting of all the hidden states (768) for each token in each sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.tensor(padded)\n",
    "attention_mask = torch.tensor(attention_mask)\n",
    "\n",
    "with torch.no_grad():\n",
    "    last_hidden_states = model(input_ids, attention_mask=attention_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since sentiment analysis is a classification task, I sliced the output to contain only the information relevant to the first token (CLS special token) of each sentence. This token can be thought of as representing the entire sentence. The sliced output is a 2D matrix where each row is a feature vector consisting of the 768 hidden units of the CLS token for the corresponding sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This matrix will be stored in a features variable and the positive or negative labels will be stored in the labels variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = last_hidden_states[0][:,0,:].numpy()\n",
    "labels = df[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Test Split\n",
    "Next, I split the data into a training set and a test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features, test_features, train_labels, test_labels = train_test_split(features, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Deep Learning Model\n",
    "\n",
    "Next, I trained several models using the training data and evaluated the accuracy using the test dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The models I trained are\n",
    "- Logistic Regression\n",
    "- SVM\n",
    "- Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "I trained the models from Scikit Learn using the default parameters, with the exception of max_iter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BayesianRidge(n_iter=10000)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Logistic Regression\n",
    "lr_clf = LogisticRegression(max_iter = MAX_ITER)\n",
    "lr_clf.fit(train_features, train_labels)\n",
    "\n",
    "# Support Vector Machine Classification\n",
    "svm_clf = SVC(max_iter = MAX_ITER)\n",
    "svm_clf.fit(train_features, train_labels)\n",
    "\n",
    "# Naive Bayes\n",
    "br_clf = BayesianRidge(n_iter = MAX_ITER)\n",
    "br_clf.fit(train_features, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "Then, I checked the accuracy of the models against the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "lr_test_score = lr_clf.score(test_features, test_labels)\n",
    "\n",
    "# Support Vector Machine Classification\n",
    "svm_test_score = svm_clf.score(test_features, test_labels)\n",
    "\n",
    "# Naive Bayes\n",
    "br_test_score = br_clf.score(test_features, test_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dummy Classifier\n",
    "In order to normalize the accuracy of each model, I used a dummy classifier provided by scikit learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/samikshakale/PycharmProjects/SentimentAnalysis/venv/lib/python3.8/site-packages/sklearn/dummy.py:131: FutureWarning: The default value of strategy will change from stratified to prior in 0.24.\n",
      "  warnings.warn(\"The default value of strategy will change from \"\n"
     ]
    }
   ],
   "source": [
    "clf = DummyClassifier()\n",
    "scores = cross_val_score(clf, train_features, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy of Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Size: 500\n",
      "Logistic Regression Model Accuracy: 0.80\n",
      "SVM Model Accuracy: 0.75\n",
      "Bayesian Ridge Model Accuracy: 0.40\n",
      "Dummy classifier score: 0.52 (+/- 0.08)\n"
     ]
    }
   ],
   "source": [
    "print(\"Dataset Size: \" + str(sample_size*2))\n",
    "print(\"Logistic Regression Model Accuracy: %0.2f\" % lr_test_score)\n",
    "print(\"SVM Model Accuracy: %0.2f\" % svm_test_score)\n",
    "print(\"Bayesian Ridge Model Accuracy: %0.2f\" % br_test_score)\n",
    "print(\"Dummy classifier score: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Program Run Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run Time (min): 6.26\n"
     ]
    }
   ],
   "source": [
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "# print(\"Start Time: \" + str(start_time))\n",
    "# print(\"End Time: \" + str(end_time))\n",
    "print(\"Run Time (min): %0.2f\" % (elapsed_time/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problems Encountered\n",
    "\n",
    "Problem: pandas read_csv function was not complying with the file containing the dataset. It was not loading the data propoerly.\n",
    "\n",
    "Solution: adding the field - encoding='utf-8' - to the read_csv function in order for pandas to load the data into a dataframe\n",
    "\n",
    "Problem: BERT supports only up to 512 tokens, which was much smaller than some of the reviews in the dataset\n",
    "\n",
    "Solution: I filtered out the nouns and verbs, choosing to keep only the adjectives and adverbs for sentiment classification.\n",
    "\n",
    "Problem: The logisitic regression model used to train the feature extraction data was timing out.\n",
    "\n",
    "Solution: After researching this issue, I decided to increase the max iterations field of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "After getting the logistic regression model to work, I added the SVM and naive bayes models to determine which of these widely-used classification models performs the best on text data and text classification. I found that the logisitc regression model works the best, however, these model should still be tested on larger dataset sizes. Additionally, part of its accuracy can be accounted for due to random choice, such as by the dummy classifier. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
